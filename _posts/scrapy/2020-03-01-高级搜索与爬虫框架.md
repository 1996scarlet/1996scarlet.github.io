---

layout:     post
title:      分布式爬虫（一）
subtitle:   高并发爬虫框架
date:       2020-03-01 17:00:00
author:     "Remilia Scarlet"
header-img: "2020/06/06/t6XpBn.jpg"
catalog: true
tags:
    - 分布式爬虫
    - scrapy
    - python
---

# 分时操作系统中的多任务编程

早期的可编程电子计算机都遵循**串行单任务**模型, 即单个程序运行期间无法同时执行其他任务, 所有的运算资源都被当前运行的程序所独占. 但串行程序在运行过程中访问外部存储设备、网络以及数据库时造成的CPU运算资源大量浪费逐渐引起了计算机科学家们的重视, 因此**多任务**的概念就出现在了早期的操作系统中.

## 并行与并发

在实际应用中, **并发（concurrency）**的出现时间要早于**并行（parallel）**. 作为对程序运行时资源的一种抽象, **进程**的概念最早出现在二十世纪六十年代. 对操作系统来说, 进程是资源分配和调度的基本单位, 其实现基础是[虚拟化技术](https://zhuanlan.zhihu.com/p/54283264)与[调度算法](https://zhuanlan.zhihu.com/p/96966239). Linux是典型的**分时操作系统**, 通过对硬件设备虚拟化将运算资源抽象为时间片, 内核通过名为**完全公平调度（Completely Fair Scheduler, CFS）**的[动态优先级抢占式](https://wyydsb.xin/other/schedule.html)时间片分配算法来调度进程, 这种微观层面的快速多任务切换最终体现为在单个处理设备上实现宏观层面的**并发多任务**处理. 在现代操作系统中, 进程、线程（包括协程）、I/O复用是典型的并发手段; 而硬件层面的CPU超线程, 是通过添加额外控制单元实现指令层面的并发执行.

真正意义上的并行计算机直到七十年代末期才被CMU设计并制造, 并行计算理论的核心就是对任务的**合理拆分**以及**数据同步**. 大到超级计算机和分布式计算机集群, 小到显卡与多核处理器, **并行多任务**能够在硬件层面真正实现同时处理, 但不容忽视的**通信成本**也会随着计算设备的增加而显著增长. 在多核处理器或显卡中, 可以通过多级高速缓存的方式实现处理器内部的数据共享; 而在分布式系统中, 也可以通过Redis、Memcached等高性能缓存中间件实现机器之间的数据同步. 但缓存容量毕竟有限, 如果发生[缓存穿透、击穿或雪崩](https://segmentfault.com/a/1190000022029639), 不仅处理效率会在短时间内显著降低, 整个系统也有宕机的风险.

与适用于I/O密集型的并发不同, 并行更适合处理计算密集型任务, 并且需要交换的数据越少则加速效果越明显. 而在实际应用中需要根据业务场景选择多任务方案, 例如: Nginx通过给每个核心绑定一个进程来实现并行, 然后在每个进程内部通过用户级线程（协程）实现并发.

> Concurrency is about dealing with lots of things at once.
>
> Parallelism is about doing lots of things at once. ---- Vicky Katara

简要总结并行与并发的区别:

* **并行（parallel）** - 在多个运算设备上同时分别执行多个任务, 是硬件层面的同时执行, 难点在于对任务的分割、合并以及数据的同步. 并行编程适用于**运算密集型**场景, 例如: 模型训练、解方程组、图形渲染等.
* **并发（concurrency）** - 在单个运算设备上同时处理多个任务, 通过软件层面的虚拟化来实现宏观时间段内的同时执行, 难点在于上下文切换成本以及高并发场景下的状态维护. 并发编程适用于**I/O密集型**场景, 例如: 网络服务器、数据库服务器、分布式爬虫等.

## 阻塞与挂起

处于**就绪态**的进程在分配到时间片之后会进入**运行态**, 但如果在运行过程中调用了`wait()`、网络I/O、文件I/O等默认阻塞的系统调用则会进入**阻塞态**. **阻塞（blocking）**的特点是**不占用时间片**, 直到内核处理完所等待的事件之后才会通知进程解除阻塞并再次进入就绪态. 而**非阻塞（non-blocking）**系统调用的特点是会**立即返回当前状态**, 即使当前内核没有准备好请求的I/O数据, 进程也会在时间片用尽之前一直处于运行态.

以`queue`模块中的队列为例, `put()`和`get()`是阻塞方法, 调用之后如果不满足条件则会一直阻塞; `put_nowait()`和`get_nowait()`是非阻塞方法, 调用之后会立刻返回结果, 如果不满条件则会抛出异常.

``` python
import queue
import threading
import time

q = queue.Queue(1)

def producer():
    while True:
        try:
            q.put_nowait("data")  # 非阻塞
        except queue.Full:
            # 即使队列满了也会继续运行到这
            print("Queue Full, Sleep")
            # 方便演示 限制频率
            time.sleep(0.2)

def customer():
    while True:
        print(q.get())  # 阻塞 每秒解除一次
        time.sleep(1)   # 方便演示 限制频率

non_blk = threading.Thread(target=producer)
non_blk.start()

blocking = threading.Thread(target=customer)
blocking.start()
```

在`non_blk`线程中调用了非阻塞API, 因此循环体可以不停地执行; 在`blocking`线程中调用了阻塞API, 因此线程暂时进入阻塞态, 直到条件满足之后解释器才通知循环体继续执行.

如果在进程初始化或运行时出现内存不足的现象, 操作系统可能会将部分进程映射到外部存储设备上, 这些进程的状态会变为**就绪挂起**或**阻塞挂起**. 由此可以看出, 是否进入阻塞或非阻塞状态取决于程序所调用的API, 而挂起操作是由操作系统所发起的.

## 用户级线程与内核级线程

进程的广泛使用显著提升了CPU利用率, 然而每个进程拥有独立虚拟地址空间的特点却导致其**创建**、**维护**和**通信**成本过高, 因此并不适用于高并发场景.

* **创建成本:** 在早期版本的系统中, `fork()`会[完全拷贝](https://www.cnblogs.com/bwangel23/p/4190043.html)父进程的数据段、堆、栈给子进程并共享代码段, 即使现代操作系统中引入了[写时拷贝技术(copy-on-write)](https://juejin.im/post/5bd96bcaf265da396b72f855), 内核也需要给子进程分配文件描述符并创建虚拟地址空间.
* **维护成本:** 为了描述与控制进程的运行过程, 内核需要为每个进程维护**进程控制块（Process Control Block, PCB）**. 在Linux中使用`task_struct`结构体存放进程的状态、标识符、调度信息、程序计数器、内存指针、寄存器上下文、I/O状态、处理器时间以及[其他必要信息](https://github.com/torvalds/linux/blob/master/include/linux/sched.h), 如此复杂的结构导致进程在切换[地址空间](https://zhuanlan.zhihu.com/p/100680594)以及[硬件上下文](https://zhuanlan.zhihu.com/p/52845869)时会占用大量CPU资源, 而且切换后**缓存**命中率的降低也会进一步影响处理效率.
* **通信成本:** 进程地址空间的独立导致除[共享内存](https://www.cnblogs.com/alantu2018/p/8991409.html)之外的进程间数据交互都需要通过内核中转, 这显著提升了**进程间通信（InterProcess Communication, IPC）**的成本. 其中**管道（PIPE）**和**命名管道（FIFO）**因为只能传递无结构数据且速度较慢而作用有限; **信号量（Semaphore）**一般只被用于状态同步; **软中断信号（Signal）**所能承载的信息太少因此很少使用; 而在分布式系统中则只能使用具有跨机器通信特点的**套接字（Socket）**和**消息队列（MQ）**.

上述问题在**线程（thread）**的概念被引入之后才得以有效緩解. 在现代操作系统中, 线程是进程中实际操作的执行者, 每个进程至少包含一个**主线程**, 这意味着操作系统给进程分配的时间片最终会被线程所使用, 因此可以说进程是CPU资源分配的最小单位. 相比于进程来说, 线程的创建成本更低、运行时需要维护的状态更少, 此外, 属于相同进程的多个线程之间可以共享该进程地址空间内的数据, 这显著降低了切换和通信的成本.

**用户级线程（User-Level Thread）**的出现时间要早于**内核级线程（Kernel-Level Thread）**. 线程的概念在诞生之后的很长一段时间内都没有被操作系统所支持, 因此早期的程序需要在进程的**用户态**自行实现对线程的创建、维护与切换等工作, 这就是所谓的用户级线程（协程）. 早期的Liunx通过`clone()`系统调用将轻量级进程（LWP）通过与父进程内存共享的方式模拟为内核级线程, 但由于效率过低且不符合POSIX threads（pthread）标准, 这种实现方式在2.6版本之后被Native POSIX Thread Library（NPTL）所取代, 通过`getconf GNU_LIBPTHREAD_VERSION`命令可以查看当前系统所用的内核级线程库.

* **用户级线程的优缺点:** 优势在于线程的调度完全在用户态进行, 线程切换时不需要频繁在用户态和内核态之间拷贝数据, 因此时间片的利用效率在**高I/O并发**的场景下会显著提升. 但缺点在于无法有效利用多核心运算资源来处理计算密集型任务, 因为无论多少个用户级线程对内核来说都是透明的, 这导致同一个进程内的所有用户级线程只能利用本该属于主线程的时间片.

* **内核级线程的优缺点:** 优势在于调度被操作系统所控制, 因此内核会根据线程的数量分配运算资源, 这让内核级线程能够有效执行**计算密集型**任务. 但内核级线程在切换时需要在用户态与内核态之间拷贝数据, 尽管这种代价远小于对进程的切换, 但再高并发场景下仍然是不可忽视的.

由于历史原因以及处于对线程安全性的考虑, 通过C语言编写的Python解释器会通过**全局解释器锁（Global Interpreter Lock, GIL）**来管理每个进程实例所创建的线程, 这意味着通过内置的`threading`模块创建的线程都是用户级线程. Python解释器通过GIL模拟了单核CPU对多线程的调度, 只有获得GIL的线程才会被执行, 并且在执行一段时间或遇到阻塞I/O之后所持有的GIL就会被释放, 这样循环下去就能够在主线程里模拟出多线程并发的效果.

尽管GIL保证了多进程下的数据同步, 但频繁的释放与竞争会导致在处理计算密集型任务时使用多线程的效率反而低于单线程. 而且由于解释器只为每个进程分配一个主线程, 这导致Python中的线程完全无法利用多核计算资源来有效处理计算密集型任务. 但我们也可以像Numpy一样利用C-Python接口直接绕开GIL, 在C语言层面利用内核级多线程. 另一种方案是将不受GIL控制的单线程异步处理模块`asyncio`当作真正的用户级线程（协程）来使用.

`asyncio`会为每个以`asyncio.run()`方式运行的线程建立[事件循环](https://docs.python.org/zh-cn/3/library/asyncio-eventloop.html)用于

并维护每个任务的状态, 当有异步任务通过`await`主动让出控制权时, `loop`会挑选一个可执行的异步任务接管控制权,

尽管`threading`和`asyncio`都适用于对网络、文件、数据库等阻塞I/O的处理, 但更低的切换成本让`asyncio`能够处理并发数量更高的场景.

根据APUE中的定义,
同步异步阻塞非阻塞

## 多路I/O复用模型

select poll epoll

# 高级搜索与简单静态爬虫

<!-- SEO是搜索引擎的核心功能之一
SEO的实现非产依赖于爬虫
搜索引擎是广义的爬虫

```
curl -A "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)" https://github.com
``` -->

## 高级搜索关键字

通过使用**高级搜索关键字**可以有效限定搜索范围, 进而过滤得到指定的内容. 常用的关键字及对应用法如下:

* **contains:文件后缀** - 确保搜索结果指向包含指定文件后缀的页面.
  * 搜索CODEX破解的单机游戏种子`codex contains:torrent`
  * 搜索软件压缩包`nginx contains:gz`
* **ext:后缀** - 确保搜索结果的URL包含指定后缀.
  * 搜索登录界面`登录 ext:jsp`
* **filetype:文件后缀** - 确保搜索结果是指定后缀的文件.
  * 搜索与人脸对齐有关的论文`face alignment filetype:pdf`
  * 搜索备案信息表格`备案 filetype:xls`
* **inanchor:文本** - 锚文本中包含指定文本
* **intitle:文本** - 网页标题包含指定文本
  * 搜索标题中包含后台登录的网站`intitle:后台登录`
* **inbody:文本** - 网页内容包含指定文本
  * 搜索内容中包含登录的选课网站`选课 inbody:登录`
* **site:域名** - 确保搜索结果来自于指定域名.
  * 搜索政府发布的某些信息`一带一路 site:gov.cn`
  * 搜索交通运输部发布的技术规范文件`技术规范 site:mot.gov.cn filetype:pdf`
  * 搜索教育部政策文档发布页面`政策 site:moe.gov.cn contains:docx`
* **language:语言代码** - 搜索指定语言的结果.
  * 搜索某些英文教程`opencv tutorial language:en`
* **ip:点分十进制地址** - 确保搜索结果来自于指定地址, 目前仅支持IPV4.
* **url:指定域** - 检查列出的域或网址是否在必应的索引内.
* **prefer:倾向内容** - 确保搜索结果更倾向于指定内容.
  * 搜索足球信息但更倾向于尤文图斯`football prefer:Juventus`
* **loc:国家代码** - 确保搜索结果来自指定国家或地区.
  * 搜索中国台湾TVCG期刊投稿信息`TVCG loc:tw`
  * 搜索中国大陆SIGGRAPH会议投稿信息`SIGGRAPH loc:cn`

**注意:** 所有关键字冒号之后都不需要添加空格; `ext`、`loc`、`url`、`prefer`关键字在中国大陆的使用受到限制.

## 高级搜索选项

**高级搜索选项**是常用[逻辑运算符](https://baike.baidu.com/item/%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6)的集合, 通过与关键字配合使用可实现更灵活的搜索, 其用法如下:

* **+搜索词** - 强制查找指定搜索词.
  * 搜索冰雪树图片而不是Bing学术`图片 +bingxueshu`
* **"搜索词"** - 搜索时完全匹配指定搜索词.
  * 搜索标题中包含完全匹配内容的结果`intitle:"face alignment"`
* **()** - 优先级分组.
  * 搜索某些网站的并集`gaze (site:thecvf.com OR site:arxiv.org)`
* **AND** 或 **&** - 查找所有搜索词.
  * 同时搜索某些关键词`gaze & face & body site:thecvf.com`
* **NOT** 或 **-** - 排除指定搜索词.
  * 屏蔽某些低质量网站的结果`python 3.8 -csdn -oschina -zol`
* **OR** 或 **\|** - 或运算指定搜索词.
  * 查询某些年份的论文`inbody:pose (2018 OR 2019) site:arxiv.org`

**注意:** 空格可以代替`AND`; 除高级搜索选项之外的其他符号如果不被包含在`""`内, 则在搜索时会被忽略;  `NOT`、`AND`、`OR`在使用时必须大写.

## 同步爬取静态资源

首先要构造网络请求, 这里我根据使用习惯选择`requests`库, 当然也可以用`urllib`或其他模块.

``` python
import requests

# 默认User-Agent为python-requests/版本号
# 配置合适的`User-Agent`可以将你的爬虫伪造成浏览器.
response = requests.get("你要爬的URL",
                        headers={"User-Agent": "浏览器标识"},
                        paras={"参数A": "值A", "参数B": "值B"})

# 验证你发送的请求头是什么
print(response.request.headers)
# 可以查看响应头包含的字段
print(response.headers)
```

得到响应结果之后, 先用`lxml`模块重构`etree`, 然后通过`XPATH`从`etree`中获得需要的数据.

``` python
from lxml import etree

# 用etree模块重构html树
html = etree.HTML(response.text)
# 获得你需要的数据
article_title = html.xpath('你的XPATH解析规则')
# 其他处理 ...
```

用**闭包**简单封装**发送请求**与**解析结果**的业务逻辑. 详细调用方式与解析细节请参见[完整代码](https://github.com/1996scarlet/1996scarlet.github.io/blob/master/code/spider/static_spider.py).

```python
def spider_builder(*, url_bulider, xpath_rules, **kwargs):

    def spider(content):
        response = requests.get(url_bulider(content), **kwargs)
        html = etree.HTML(response.text)
        return (html.xpath(rule) for rule in xpath_rules)

    return spider
```

**注意:** 限制爬取频率可以让你的爬虫更像真实的用户, 防止被目标网站封禁IP地址.

## 单线程异步并发爬虫

[aiohttp](https://docs.aiohttp.org/en/stable/client_quickstart.html#make-a-request)

为了对进程资源进一步细分, 线程就出现了
线程分为内核级线程(线程)和用户级线程(协程)
内核级线程就是我们常用的pthread, 他的调度完全取决于内核,
用户级线程就是async..await. 他的调度受用户(程序员)控制,

可以通过队列实现任务的有序处理

# 爬虫框架

[爬虫](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711)是能够批量发送**网络请求**并对返回的数据进行**解析与存储**的工具. 为了更高效地获取网络中的内容, 在设计爬虫程序时不仅需要通过更换网络代理、伪装`User-Agent`等方式来应对网站的反爬虫机制, 还需要借助异步`IO`、分布式等技术手段提升资源的利用率. 常用的**爬虫框架**会集成上述模块并提供`high-level`的调用接口, 因此可以显著提升爬虫程序的编写效率. [Scrapy](https://scrapy.org/)就是一个用于批量提取网站结构化数据的应用程序框架, 目前被广泛应用于数据挖掘、历史存档、文本信息处理等领域.

## 安装与简单应用

本文的测试环境, 注意`scrapy 2.0`以及之后的版本仅支持`python 3`:

* `Ubuntu 20.04 LTS`
* `python 3.8.2`
* `Scrapy 2.1.0`
* `scrapy-redis 0.6.8`

鉴于国内大部分网络运营商限制了对`pypi.org`的访问, 因此建议通过替换`pip`软件源为[国内镜像](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)的方式来提升模块下载速度.

``` bash
pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

修改为国内软件源之后, 通过`pip`直接安装.

``` bash
pip3 install Scrapy scrapy-redis
```

## 案例1: 批量下载论文

``` C
gaze estimation filetype:pdf (site:openaccess.thecvf.com OR site:arxiv.org)
```

## 案例2: 表情包爬取

## 案例3: 视频评论爬取

# 参考内容

* [Advanced search keywords](https://help.bing.microsoft.com/#apex/18/en-US/10001/-1)
* [Google hacking](https://www.ddosi.com/b88/)
* [XPATH教程](https://www.runoob.com/xpath/xpath-tutorial.html)
