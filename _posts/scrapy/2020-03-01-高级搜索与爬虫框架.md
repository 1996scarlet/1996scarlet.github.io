---

layout:     post
title:      分布式爬虫（一）
subtitle:   高并发爬虫框架
date:       2020-03-01 17:00:00
author:     "Remilia Scarlet"
header-img: "2020/06/06/t6XpBn.jpg"
catalog: true
tags:
    - 分布式爬虫
    - scrapy
    - python
---

# 分时操作系统中的多任务编程

早期的可编程电子计算机都遵循**串行单任务**模型, 即单个程序运行期间无法同时执行其他任务, 所有的运算资源都被当前运行的程序所独占. 但串行程序在运行过程中访问外部存储设备、网络以及数据库时造成的CPU运算资源大量浪费逐渐引起了计算机科学家们的重视, 因此**多任务**的概念就出现在了早期的操作系统中.

在实际应用中, **并发 (concurrency)** 的出现时间要早于**并行 (parallel)**. 作为对程序运行时资源的一种抽象, **进程**的概念最早出现在二十世纪六十年代. 对操作系统来说, 进程是资源分配和调度的基本单位, 其实现基础是[虚拟化技术](https://wiki.mbalib.com/wiki/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF)与[调度算法](https://zhuanlan.zhihu.com/p/96966239). Linux是典型的**分时操作系统**, 通过对硬件设备虚拟化将运算资源抽象为时间片, 内核通过名为**完全公平调度 (Completely Fair Scheduler, CFS)** 的[动态优先级抢占式](https://wyydsb.xin/other/schedule.html)时间片分配算法来调度进程, 这种微观层面的快速多任务切换最终体现为在单个处理设备上实现宏观层面的**并发多任务**处理. 在现代操作系统中, 进程、线程(包括协程)、I/O复用是典型的并发手段; 而硬件层面的CPU超线程, 是通过添加额外控制单元实现指令层面的并发执行.

真正意义上的并行计算机直到七十年代末期才被CMU设计并制造, 并行计算理论的核心就是对任务的**合理拆分**以及**数据同步**. 大到超级计算机和分布式计算机集群, 小到显卡与多核处理器, **并行多任务**能够在硬件层面真正实现同时处理, 但不容忽视的**通信成本**也会随着计算设备的增加而显著增长. 在多核处理器或显卡中, 可以通过多级高速缓存的方式实现处理器内部的数据共享; 而在分布式系统中, 也可以通过Redis、Memcached等高性能缓存中间件实现机器之间的数据同步. 但缓存容量毕竟有限, 如果发生[缓存穿透、击穿或雪崩](https://segmentfault.com/a/1190000022029639), 不仅处理效率会在短时间内显著降低, 整个系统也有宕机的风险.

与适用于I/O密集型的并发不同, 并行更适合处理计算密集型任务, 并且需要交换的数据越少则加速效果越明显. 而在实际应用中需要根据业务场景选择多任务方案, 例如: Nginx通过给每个核心绑定一个进程来实现并行, 然后在每个进程内部通过用户级线程(协程)实现并发.

> Concurrency is about dealing with lots of things at once.
>
> Parallelism is about doing lots of things at once. ---- Vicky Katara

简要总结并行与并发的区别:

* **并行 (parallel)** - 在多个运算设备上同时分别执行多个任务, 是硬件层面的同时执行, 难点在于对任务的分割、合并以及数据的同步. 并行编程适用于**运算密集型**场景, 例如: 模型训练、解方程组、图形渲染等.
* **并发 (concurrency)** - 在单个运算设备上同时处理多个任务, 通过软件层面的虚拟化来实现宏观时间段内的同时执行, 难点在于上下文切换成本以及高并发场景下的状态维护. 并发编程适用于**I/O密集型**场景, 例如: 网络服务器、数据库服务器、分布式爬虫等.

同步异步阻塞非阻塞

## 多进程模型

## 多路I/O复用模型

## 多线程模型

线程包括用户级与内核级

# 高级搜索与简单静态爬虫

<!-- SEO是搜索引擎的核心功能之一
SEO的实现非产依赖于爬虫
搜索引擎是广义的爬虫

```
curl -A "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)" https://github.com
``` -->

## 高级搜索关键字

通过使用**高级搜索关键字**可以有效限定搜索范围, 进而过滤得到指定的内容. 常用的关键字及对应用法如下:

* **contains:文件后缀** - 确保搜索结果指向包含指定文件后缀的页面.
  * 搜索CODEX破解的单机游戏种子`codex contains:torrent`
  * 搜索软件压缩包`nginx contains:gz`
* **ext:后缀** - 确保搜索结果的URL包含指定后缀.
  * 搜索登录界面`登录 ext:jsp`
* **filetype:文件后缀** - 确保搜索结果是指定后缀的文件.
  * 搜索与人脸对齐有关的论文`face alignment filetype:pdf`
  * 搜索备案信息表格`备案 filetype:xls`
* **inanchor:文本** - 锚文本中包含指定文本
* **intitle:文本** - 网页标题包含指定文本
  * 搜索标题中包含后台登录的网站`intitle:后台登录`
* **inbody:文本** - 网页内容包含指定文本
  * 搜索内容中包含登录的选课网站`选课 inbody:登录`
* **site:域名** - 确保搜索结果来自于指定域名.
  * 搜索政府发布的某些信息`一带一路 site:gov.cn`
  * 搜索交通运输部发布的技术规范文件`技术规范 site:mot.gov.cn filetype:pdf`
  * 搜索教育部政策文档发布页面`政策 site:moe.gov.cn contains:docx`
* **language:语言代码** - 搜索指定语言的结果.
  * 搜索某些英文教程`opencv tutorial language:en`
* **ip:点分十进制地址** - 确保搜索结果来自于指定地址, 目前仅支持IPV4.
* **url:指定域** - 检查列出的域或网址是否在必应的索引内.
* **prefer:倾向内容** - 确保搜索结果更倾向于指定内容.
  * 搜索足球信息但更倾向于尤文图斯`football prefer:Juventus`
* **loc:国家代码** - 确保搜索结果来自指定国家或地区.
  * 搜索中国台湾TVCG期刊投稿信息`TVCG loc:tw`
  * 搜索中国大陆SIGGRAPH会议投稿信息`SIGGRAPH loc:cn`

**注意:** 所有关键字冒号之后都不需要添加空格; `ext`、`loc`、`url`、`prefer`关键字在中国大陆的使用受到限制.

## 高级搜索选项

**高级搜索选项**是常用[逻辑运算符](https://baike.baidu.com/item/%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%AC%A6)的集合, 通过与关键字配合使用可实现更灵活的搜索, 其用法如下:

* **+搜索词** - 强制查找指定搜索词.
  * 搜索冰雪树图片而不是Bing学术`图片 +bingxueshu`
* **"搜索词"** - 搜索时完全匹配指定搜索词.
  * 搜索标题中包含完全匹配内容的结果`intitle:"face alignment"`
* **()** - 优先级分组.
  * 搜索某些网站的并集`gaze (site:thecvf.com OR site:arxiv.org)`
* **AND** 或 **&** - 查找所有搜索词.
  * 同时搜索某些关键词`gaze & face & body site:thecvf.com`
* **NOT** 或 **-** - 排除指定搜索词.
  * 屏蔽某些低质量网站的结果`python 3.8 -csdn -oschina -zol`
* **OR** 或 **\|** - 或运算指定搜索词.
  * 查询某些年份的论文`inbody:pose (2018 OR 2019) site:arxiv.org`

**注意:** 空格可以代替`AND`; 除高级搜索选项之外的其他符号如果不被包含在`""`内, 则在搜索时会被忽略;  `NOT`、`AND`、`OR`在使用时必须大写.

## 同步爬取静态资源

首先要构造网络请求, 这里我根据使用习惯选择`requests`库, 当然也可以用`urllib`或其他模块.

``` python
import requests

# 默认User-Agent为python-requests/版本号
# 配置合适的`User-Agent`可以将你的爬虫伪造成浏览器.
response = requests.get("你要爬的URL",
                        headers={"User-Agent": "浏览器标识"},
                        paras={"参数A": "值A", "参数B": "值B"})

# 验证你发送的请求头是什么
print(response.request.headers)
# 可以查看响应头包含的字段
print(response.headers)
```

得到响应结果之后, 先用`lxml`模块重构`etree`, 然后通过`XPATH`从`etree`中获得需要的数据.

``` python
from lxml import etree

# 用etree模块重构html树
html = etree.HTML(response.text)
# 获得你需要的数据
article_title = html.xpath('你的XPATH解析规则')
# 其他处理 ...
```

用**闭包**简单封装**发送请求**与**解析结果**的业务逻辑. 详细调用方式与解析细节请参见[完整代码](https://github.com/1996scarlet/1996scarlet.github.io/blob/master/code/spider/static_spider.py).

```python
def spider_builder(*, url_bulider, xpath_rules, **kwargs):

    def spider(content):
        response = requests.get(url_bulider(content), **kwargs)
        html = etree.HTML(response.text)
        return (html.xpath(rule) for rule in xpath_rules)

    return spider
```

**注意:** 限制爬取频率可以让你的爬虫更像真实的用户, 防止被目标网站封禁IP地址.

## 单线程异步并发爬虫

[aiohttp](https://docs.aiohttp.org/en/stable/client_quickstart.html#make-a-request)

为了对进程资源进一步细分, 线程就出现了
线程分为内核级线程(线程)和用户级线程(协程)
内核级线程就是我们常用的pthread, 他的调度完全取决于内核,
用户级线程就是async..await. 他的调度受用户(程序员)控制,

可以通过队列实现任务的有序处理

# 爬虫框架

[爬虫](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711)是能够批量发送**网络请求**并对返回的数据进行**解析与存储**的工具. 为了更高效地获取网络中的内容, 在设计爬虫程序时不仅需要通过更换网络代理、伪装`User-Agent`等方式来应对网站的反爬虫机制, 还需要借助异步`IO`、分布式等技术手段提升资源的利用率. 常用的**爬虫框架**会集成上述模块并提供`high-level`的调用接口, 因此可以显著提升爬虫程序的编写效率. [Scrapy](https://scrapy.org/)就是一个用于批量提取网站结构化数据的应用程序框架, 目前被广泛应用于数据挖掘、历史存档、文本信息处理等领域.

## 安装与简单应用

本文的测试环境, 注意`scrapy 2.0`以及之后的版本仅支持`python 3`:

* `Ubuntu 20.04 LTS`
* `python 3.8.2`
* `Scrapy 2.1.0`
* `scrapy-redis 0.6.8`

鉴于国内大部分网络运营商限制了对`pypi.org`的访问, 因此建议通过替换`pip`软件源为[国内镜像](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)的方式来提升模块下载速度.

``` bash
pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

修改为国内软件源之后, 通过`pip`直接安装.

``` bash
pip3 install Scrapy scrapy-redis
```

## 案例1: 批量下载论文

``` C
gaze estimation filetype:pdf (site:openaccess.thecvf.com OR site:arxiv.org)
```

## 案例2: 表情包爬取

## 案例3: 视频评论爬取

# 参考内容

* [Advanced search keywords](https://help.bing.microsoft.com/#apex/18/en-US/10001/-1)
* [Google hacking](https://www.ddosi.com/b88/)
* [XPATH教程](https://www.runoob.com/xpath/xpath-tutorial.html)
