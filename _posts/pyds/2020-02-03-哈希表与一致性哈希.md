---

layout:     post
title:      高级数据结构（三）
subtitle:   哈希表与一致性哈希
date:       2020-02-03 10:00:00
author:     "Remilia Scarlet"
header-img: "2020/06/06/t6XpBn.jpg"
catalog: true
mathjax: true
tags:
    - 数据结构
---

# 哈希表

[哈希表（Hash Table）](https://baike.baidu.com/item/%E5%93%88%E5%B8%8C%E8%A1%A8/5981869)是经典的[键值存储（Key-Value）](https://img.xileso.top/baike-%E9%94%AE-%E5%80%BC%E5%AD%98%E5%82%A8)数据结构, 其特点是随机访问时需要先通过**哈希函数**计算每个键所对应的哈希值, 再通过**索引映射函数**得到哈希值在哈希表中的真实索引, 最后根据键的真实索引得到其对应的值.

令哈希函数为$hash()$, 索引映射函数为$index()$, 则在给定的哈希表$T$中, 得到键$k$所对应的值$v$的过程可以描述如下:

\begin{equation}
k_{hash} = hash(k)
\end{equation}

\begin{equation}
k_{index} = index(k_{hash})
\end{equation}

\begin{equation}
T[k] = T.get(k_{index}) = v
\end{equation}

其中, $index()$的实现方式与哈希表的结构和**哈希冲突**的解决方法有关. 以链地址形式的哈希表为例, $index()$首先会对$k_{hash}$以数组长度取模得到对应链表的首地址, 然后通过遍历该链表得到真实值.

哈希表的设计思路是**空间换时间**, 令$T$为节点数量为$s$的哈希表, 设其中非空节点的数量为$m$, 则**负载因子（Load Factor）**可按照如下公式计算:

\begin{equation}
F_{load} = m/s
\end{equation}

相比于空间利用率接近100%的[红黑树](https://zhuanlan.zhihu.com/p/31805309)来说, 哈希表会在负载因子大于阈值时进行扩容并重新映射所有数据, 这样能保证插入和查找数据的时间复杂度尽不会随着哈希冲突出现的次数而显著增加. 设计良好的哈希表能够以$O(1)$时间复杂度实现对数据的查找, 但如果使用的哈希函数无法有效地让键离散化, 那么严重的哈希冲突会导致哈希表的查找时间复杂度退化为$O(n)$的线性表或$O(logn)$的红黑树.

## 常用哈希算法

哈希函数是基于[哈希算法](https://www.zhihu.com/question/26762707)实现的[单向函数](https://baike.baidu.com/item/%E5%8D%95%E5%90%91%E5%87%BD%E6%95%B0), 其特点是输入为无限空间内的任意对象, 但输出却被限定为有限空间内指定长度的整数, 并且很难通过结果推算出原始输入. 哈希算法的这些特点让其在[密码学](https://www.esat.kuleuven.be/cosic/publications/article-1532.pdf)、[数据校验](https://baike.baidu.com/item/MD5)、[负载均衡](https://www.bilibili.com/video/BV1Hs411j73w)等领域得到了广泛的应用. 根据[PlanetMath](https://planetmath.org/hashing)中的定义, 优秀的哈希算法应该满足以下性质:

**防碰撞 -** 理想情况下的哈希函数应属于[单射](https://baike.baidu.com/item/%E5%8D%95%E5%B0%84), 即对$\forall x,y \in $输入空间, 在$x \neq y$的情况下都满足$hash(x) \neq hash(y)$. 然而在实际应用中, 哈希函数的输入空间一般远大于输出空间, 因此根据[抽屉原理](https://baike.baidu.com/item/%E6%8A%BD%E5%B1%89%E5%8E%9F%E7%90%86/233776), 在输入的数据足够多的情况下必然会发生碰撞.

在哈希表中, 可以通过开放地址法或链地址法处理发生冲突的数据; 而在安全性要求较高的密码学领域, 攻击者一旦掌握了构造哈希冲突数据的方法就能够轻易破解加密信息, 因此目前常用的加密哈希算法在设计时都倾向于提升破解所需要的成本, 例如: Google提出的SHA-1破解方法需要在单个GPU上运算100年左右, 而暴力破解比特币所使用的SHA-256算法则需要在单个矿机上运行数十亿年.

但在某些特殊的应用场景下, 例如: 通过"姓名+身份证号"查询考试信息, 如果输入值的范围能够事先能够确定且输入空间小于等于输出空间, 则能够通过GNU提供的[gperf](https://www.gnu.org/software/gperf/)工具设计出不会发生冲突的**完美哈希函数**.

**防篡改 -** 被用于数据校验的哈希算法需要满足对数据敏感的性质, 即使微小的输入变化也应该让输出的结果显著不同. 例如: 在拥有大量不可信机器的[对等网络](https://baike.baidu.com/item/%E5%AF%B9%E7%AD%89%E7%BD%91%E7%BB%9C)中下载数据时, 为了防止数据出错导致整个文件被重新下载, 需要将大文件分割为4K大小的数据块. 在下载数据之前需要先从可信数据源获取正确的**根哈希**, 然后下载由每个块对应的哈希值所构成的哈希表, 对哈希表中所有哈希值按顺序拼接成的字符串再次哈希, 并将其与根哈希对比就可以验证表中数据的正确性, 从而验证整个文件的正确性. 这种通过[墨克树](https://img.xileso.top/wiki/%E5%93%88%E5%B8%8C%E6%A0%91)校验大文件的方法还被应用于数字货币区块信息的防篡改中.

尽管安全哈希算法（Secure Hash Algorithm, SHA）是不可逆的, 但通过暴力枚举构建[彩虹表](https://www.zhihu.com/question/19790488)或其他类似的手段仍然能够实现信息窃取, 例如: [针对SHA-1和MD5的反向查询网站](https://www.cmd5.com/). 目前Git仍在使用SHA-1来计算每次提交的唯一标识和数据校验凭证, 尽管在构造哈希值时使用了元信息混淆, 但由于SHA-1算法的破解成本日趋降低, Git从2.2版本开始考虑逐步向更安全的SHA-256算法迁移, 更多细节请参见[Linus对Git安全性的回应](https://public-inbox.org/git/CA+dhYEViN4-boZLN+5QJyE7RtX+q6a92p0C2O6TA53==BZfTrQ@mail.gmail.com/T/).

**高效率 -** 上面提到的SHA和MD都属于计算过程复杂但安全性非常高的加密哈希算法, 但考虑到在哈希表中一般不需要考虑所存储数据的安全性, 并且每次查找、插入、删除数据时都需要使用键的哈希值作为索引来随机访问, 因此在设计专门用于哈希表的哈希算法时应该侧重于提升计算效率. **位运算**是常用的哈希函数计算效率优化方法, 也可以通过**结果缓存**的方式减少计算的次数.

```python
from functools import lru_cache
# 省略类相关代码
@lru_cache
def _hash(self, key):
    hashed = self._hash_func(key)
    # 等价于 hashed % self._size
    return self._size - 1 & hashed
```

在Redis中最早使用计算简单的[DJBX33A](https://www.sciencedirect.com/topics/computer-science/hash-collision)作为字符串哈希算法, 并使用[32 bit Mix Functions](https://gist.github.com/badboy/6267743)作为整数哈希算法. 后来为了提升代码的可维护性, 这两种算法在3.0版本之后被统一替换为碰撞抗性更好的[MurMurHash2](https://github.com/aappleby/smhasher/blob/master/src/MurmurHash2.cpp)算法. 但随着Redis以缓存中间件的身份被大量应用于Web服务中, 对[哈希洪水攻击](https://www.zhihu.com/question/286529973)的防范能力也日渐成为了哈希算法的选择标准, 最终在5.0版本之后又将哈希算法替换为了更安全且针对短字符串做了效率优化的[SipHash](https://131002.net/siphash/)算法. 总之, 哈希算法的选择一定要紧密联系实际的应用场景.

## 哈希冲突解决方案

在实际应用中, 哈希函数的输入空间一般要远大于输出空间, 因此当输入数据达到一定规模时就不可避免地会发生哈希冲突. 现假设哈希函数$hash()$的输出$h$服从均匀分布, 且输出空间的容量为$s$, 即对每个不相同的输入$k$来说, 其映射到输出空间内每个值的概率$P_k$都可以表示为:

\begin{equation}
P_k = \frac{1}{s}
\end{equation}

令$n$次输入时**不发生**冲突的概率为$\hat{P_n}$, 则在$n< s$的情况下:

\begin{equation}
\hat{P_n} = \prod^{n}_{i=1}(1-(i-1)P_k)
\end{equation}

根据[麦克劳林公式](https://baike.baidu.com/item/%E9%BA%A6%E5%85%8B%E5%8A%B3%E6%9E%97%E5%85%AC%E5%BC%8F), 当$-(i-1)P_k$足够小时:

\begin{equation}
1-(i-1)P_k = e^{-(i-1)P_k}
\end{equation}

因此$\hat{P_n}$可进一步化简为:

\begin{equation}
\hat{P_n} = \prod^{n}_{i=1}e^{-(i-1)P_k} = e^{-\frac{n(n-1)}{2}P_k}
\end{equation}

则$n$次输入时**发生**冲突的概率$P_n$可以表示为:

\begin{equation}
P_n = 1 - \hat{P_n} = 1 - e^{\frac{n(1-n)}{2s}}
\end{equation}

假设$hash$的输出空间为$2^{32}$范围内的整数所构成的集合, 则在$n>77162$时, 发生冲突的概率就会大于50%; 当$n>92677$时, 发生冲突的概率就会飙升到99.99%, 因此在实现哈希表时必须要想办法解决哈希冲突. 根据是否需要占用额外空间, 可以将哈希表中常用的哈希冲突解决方案分为以下两类:

* 以**开放定址法**和**再哈希法**为代表的, 直接在原始数组上寻找空闲空间的方法.
* 以**链地址法**和**公共溢出区法**为代表的, 开辟额外空间存储冲突数据的方法.

开放定址法的核心思想是在冲突节点的周围按照线性探测、二次探测、随机哈希探测等方式寻找空闲的节点, 其优点是不额外构造用于存储冲突数据的数据结构, 因此空间利用率高、序列化较容易、适合构建完美哈希; 但缺点是查找和删除的逻辑十分复杂, 经常需要多次探测才能确定要查询的数据是否在表中, 因此当表中存在大量冲突数据时会导致整体处理效率明显下降.

而与之相对应的链地址法则是在数组的每个节点中, 以链表的形式来记录所有映射到该位置的数据. 其优点是在数据频繁添加与删除的场景下的平均时间复杂度要低于开放地址法, 因此JDK、Redis、Python等项目中提供的哈希结构普遍使用链地址法来实现. 此外, 链地址法也具有很强的灵活性, 扩展结构并不局限于单向链表, 例如: JDK中的HashMap会根据每个节点所存储的冲突数据数量, 将扩展结构在链表和红黑树之间切换.

链地址的实现也相对简单, 在建立哈希表时要给每个节点初始化为链表.

```python
self._entry = [[] for i in range(self._size)]
```

插入数据时需要先找到对应的链表, 如果该键已存在则直接更新.

```python
def _insert(self, key, value):
    # 获取key对应的哈希值
    index = self._hash(key)
    # 根据哈希值获得对应的链表索引
    index_list = self._entry[index]

    # 如果该key已经存在就更新value
    for i, item in enumerate(index_list):
        if item[0] == key:
            index_list[i] = (key, value)
            return
    # 如果该key不存在就新增(key, value)元组
    index_list.append((key, value))
```

最后重载运算符, 更多实现细节参见[完整代码](https://github.com/1996scarlet/1996scarlet.github.io/blob/master/code/hash/hash_table.py).

## 哈希表结构优化

Python3.7版本之后的字典结构要优于普通哈希表, 因为他的索引是整数, 哈希值和kv存在链表里, 只需要对整数数组做rehash, 因此数据量特别大的时候效率也不会明显降低

实现字典

JDK的优化 完全随即hash的情况下, 每个节点所对应的list的长度服从泊松分布, 大于8时概率为0.0000006, 转换为红黑树, 节省空间

求hash时用位运算加速

负载因子, 为什么选0.75, 最优解是ln2也就是0.693

令事件A为节点$Q$被插入数据, 则在完全随机生成哈希值的情况下, 每次向$T$中添加数据时事件A发生的概率为:

\begin{equation}
P_A=\frac{1}{s}
\end{equation}

现在向$T$中添加$n$个数据, 用$X$表示这$n$次独立重复的实验中事件A发生的次数, 显然服从[二项分布](https://baike.baidu.com/item/%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83):

\begin{equation}
P_{X=k} = C(n, k)P_A^k(1-P_A)^{n-k}
\end{equation}

那么$n$次实验中事件A恰好发生0次的概率为:

\begin{equation}
P_{X=0} = (\frac{s-1}{s})^{n}
\end{equation}

这个对于节点$Q$成立的结论可以推广到$T$中的所有节点, 为了保证

当添加数据的数量小于$log(2)/log(s/(s - 1))$时, 满足$P_{X=0}>0.5$来保证CPU分支预测时缓存的命中率, 也就是让不发生哈希冲突的概率大于发生

因此代入

现在问题就转化为求

由于CPU中分支预测的阈值为0.5, 因此

每个节点被插入的概率为$P=\frac{1}{s}$

则$n$次, 事件A恰好发生0次的概率:

每次插入新键时每个节点

每个节点发生哈希冲突的概率服从二项分布, 而每个节点发生哈希冲突的次数服从泊松分布. 那么我们

\begin{equation}
P(0) = C(n, 0) * 1/s
\end{equation}

相比于空间利用率

是一种空间换时间的数据结构, 能够有效查找数据

其中, **计算效率**与结果的**离散程度**是评价哈希表所使用的哈希函数的最重要标准.

好的哈希函数能够让映射的结果尽量分散

最近看到mysql的hash表，发现一个特点。
当hash表满的时候，hash表size总是扩展成一个素数

假设hash表大小为size，这是一个合数，即有size=a*n。当有hash值为hashcode，且hashcode = b*n.
则hashcode取模之后为
hashcode = hashcode%size = hashcode - (hashcode / size) *size = hashcode - (b/a)* size
因为a是固定的，那么上面的hashcode的取值只有b种可能，这样显然会增加冲突的概率。

函数设计得好就不用对质数取模, JDK用的算

# 一致性哈希

应用场景: 缓存集群的负载均衡

例如: 3千万张图片, N台缓存服务器组成集群

server_num = hash(img_name) % N

添加服务器, N变化时, 同一张图片的server_num的值会变化, 所有的缓存都要重新配置

设计一种动态伸缩算法 增加或删除服务器时对原有系统的改动有限 降低维护成本 只需要修改修改哈希环上的下一个真实节点的缓存数据 而普通取模算法则需要修改全部服务器

## 哈希环与虚拟节点

理想情况 均匀分布 映射到2^32哈希环中

图1:

但实际上分布不均匀 哈希偏置
而且某台服务器突然宕机会导致下一个节点承受双倍流量
所以引入虚拟节点

图2:

## 实现细节

用python模拟实现负载均衡
map 红黑树 保证插入有序

寻找大于当前hash值的第一个节点

注意处理边界

# 参考内容

* [Hash Tables via USTC](http://staff.ustc.edu.cn/~csli/graduate/algorithms/book6/chap12.htm)
* [History of Hash Functions](https://www.esat.kuleuven.be/cosic/publications/article-1532.pdf)
* [ocert-2012-001](http://ocert.org/advisories/ocert-2012-001.html)
